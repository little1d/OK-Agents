{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, ModelType\n",
    "from camel.loaders import UnstructuredIO\n",
    "from camel.storages import Neo4jGraph\n",
    "from camel.retrievers import AutoRetriever\n",
    "from camel.embeddings import OpenAIEmbedding, SentenceTransformerEncoder\n",
    "from camel.types import StorageType, EmbeddingModelType\n",
    "from camel.agents import ChatAgent, KnowledgeGraphAgent\n",
    "from camel.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_ai camel_retriever\n",
    "camel_retriever = AutoRetriever(\n",
    "    vector_storage_local_path=\"local_data/embedding_storage\",\n",
    "    url_and_api_key=(\n",
    "        \"./milvus_demo.db\",\n",
    "        \"\",\n",
    "        # \"http://10.140.52.87:19530\",  # Your Milvus connection URL\n",
    "        # \"\",  # Your Milvus token\n",
    "    ),\n",
    "    storage_type=StorageType.MILVUS,\n",
    "    embedding_model=OpenAIEmbedding(url=OPENAI_API_BASE, api_key=OPENAI_API_KEY),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "camel_retriever = AutoRetriever(\n",
    "    vector_storage_local_path=\"local_data/embedding_storage\",\n",
    "    url_and_api_key=(\n",
    "        \"./milvus_demo.db\",\n",
    "        \"\",\n",
    "        # \"http://10.140.52.87:19530\",  # Your Milvus connection URL\n",
    "        # \"\",  # Your Milvus token\n",
    "    ),\n",
    "    storage_type=StorageType.MILVUS,\n",
    "    embedding_model=SentenceTransformerEncoder(\n",
    "        model_name=\"intfloat/e5-large-v2\"\n",
    "    ),  # 1.3G\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set one user query\n",
    "query = \"what's the relationship between Mistral Large 2 and Mistral AI? What kind of feature does Mistral Large 2 has?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Original Query': \"what's the relationship between Mistral Large 2 and Mistral AI? What kind of feature does Mistral Large 2 has?\", 'Retrieved Context': ['Large Enough\\n\\nToday, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.\\n\\nResearch\\n\\nJul 24, 2024\\n\\nMistral AI team']}\n"
     ]
    }
   ],
   "source": [
    "# Get related content by using vector retriever\n",
    "vector_result = camel_retriever.run_vector_retriever(\n",
    "    query=query,\n",
    "    contents=\"https://mistral.ai/news/mistral-large-2407/\",\n",
    ")\n",
    "\n",
    "# Show the result from vector search\n",
    "print(vector_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's the relationship between Mistral Large 2 and Mistral AI? What kind of feature does Mistral Large 2 has?\n"
     ]
    }
   ],
   "source": [
    "print(vector_result[\"Original Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse content from the specified URL and create knowledge graph data\n",
    "\n",
    "# Parse content from mistral website and create knowledge graph data by using\n",
    "# the Knowledge Graph Agent, store the information into graph database\n",
    "uio = UnstructuredIO()\n",
    "\n",
    "elements = uio.parse_file_or_url(\n",
    "    input_path=\"https://mistral.ai/news/mistral-large-2407\"\n",
    ")\n",
    "chunk_elements = uio.chunk_elements(chunk_type=\"chunk_by_title\", elements=elements)\n",
    "\n",
    "graph_elements = []\n",
    "for chunk in chunk_elements:\n",
    "    graph_elements = kg_agent.run(chunk, parse_graph_elements=True)\n",
    "    n4j.add_graph_elements(graph_elements=[graph_element])\n",
    "    graph_elements.append(graph_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an element from user query\n",
    "query_element = uio.create_element_from_text(text=query, element_id=\"1\")\n",
    "\n",
    "# Let Knowledge Graph Agent extract node and relationship information from the query\n",
    "ans_element = kg_agent.run(query_element, parse_graph_elements=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the entity got from query in the knowledge graph storage content\n",
    "kg_result = []\n",
    "for node in ans_element.nodes:\n",
    "    n4j_query = f\"\"\"\n",
    "MATCH (n {{id: '{node.id}'}})-[r]->(m)\n",
    "RETURN 'Node ' + n.id + ' (label: ' + labels(n)[0] + ') has relationship ' + type(r) + ' with Node ' + m.id + ' (label: ' + labels(m)[0] + ')' AS Description\n",
    "UNION\n",
    "MATCH (n)<-[r]-(m {{id: '{node.id}'}})\n",
    "RETURN 'Node ' + m.id + ' (label: ' + labels(m)[0] + ') has relationship ' + type(r) + ' with Node ' + n.id + ' (label: ' + labels(n)[0] + ')' AS Description\n",
    "\"\"\"\n",
    "    result = n4j.query(query=n4j_query)\n",
    "    kg_result.extend(result)\n",
    "\n",
    "# Show the result from knowledge graph database\n",
    "print(kg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine result from vector search and knowledge graph entity search\n",
    "comined_results = str(vector_result) + \"\\n\".join(kg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an assistant agent to answer questions based on the retrieved context:\n",
    "sys_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=\"CAMEL Agent\",\n",
    "    content=\"\"\"You are a helpful assistant to answer question,\n",
    "        I will give you the Original Query and Retrieved Context,\n",
    "    answer the Original Query based on the Retrieved Context.\"\"\",\n",
    ")\n",
    "camel_agent = ChatAgent(system_message=sys_msg, model=deepseek)\n",
    "\n",
    "# Pass the retrieved information to agent\n",
    "user_prompt = f\"\"\"\n",
    "The Original Query is {query}\n",
    "The Retrieved Context is {comined_results}\n",
    "\"\"\"\n",
    "\n",
    "user_msg = BaseMessage.make_user_message(role_name=\"CAMEL User\", content=user_prompt)\n",
    "\n",
    "# Get response\n",
    "agent_response = camel_agent.step(user_msg)\n",
    "\n",
    "print(agent_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
